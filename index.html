
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="./css/style.css">
    <title>AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds
</title>
  </head>
  <body>
    <header class="page-header" role="banner">
      <h1 class="project-name">AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds
</h1>
      <div class="btn-group-flex">
        <a href="" class="btn"><img src="./images/paper.svg" class="icon-paper"/>Paper</a>
        <a href="https://github.com/Runsong123/AdaFit" class="btn"><img src="./images/paper.svg" class="icon-paper"/>code</a>
        <!-- <a href="https://github.com/Runsong123/AdaFit" class="btn"><img src="./images/paper.svg" class="icon-paper"/></a> -->
        <!-- <a href="" class="btn"><img src="assets/images/icons/dataset.svg" class="icon-dataset"/>H3DS dataset</a> -->
      </div>
      <div class="authors">
        <p class="authors__name">Runsong zhu<sup>1</sup></p>,
        <p class="authors__name">Liu Yuan *(equal contribution)<sup>2</sup></p>,
        <p class="authors__name">Zhen Dong<sup>1</sup></p>,
        <p class="authors__name">Tengping jiang<sup>1</sup></p>,
        <p class="authors__name">Yuan wang<sup>1</sup></p>,
        <p class="authors__name">Wenping Wang<sup>2</sup></p>,
        <p class="authors__name">Bisheng Yang<sup>1</sup></p>
      </div>
      <div class="affiliations">
        <p class="affiliation__name"><sup>1</sup>Wuhan University</p>,
        <p class="affiliation__name"><sup>2</sup>the University of Hong Kong</p>
        <!-- <p class="affiliation__name"><sup>3</sup>Institut de Robòtica i Informàtica Industrial, CSIC-UPC</p> -->
      </div>
    </header>

    <main id="content" class="main-content" role="main">
      <p><img src="./images/teaser_new_colorbar.png" alt="" /></p>

<h2 id="abstract">Abstract</h2>

<p>This paper presents a neural network for robust normal estimation on point clouds, named AdaFit, that can deal with point clouds with noise and density variations. Existing works use a network to learn point-wise weights for weighted least squares surface fitting to estimate the normal, which has difficulty in finding accurate normals in complex regions or containing noisy points. By analyzing the step of weighted least squares surface fitting, we find that it is hard to determine the polynomial order of the fitting surface and the fitting surface is sensitive to outliers. To address these problems, we propose a simple yet effective solution that adds an additional offset prediction to improve the quality of normal estimation. Furthermore, in order to take advantage of points from different neighborhood sizes, a novel Cascaded Scale Aggregation layer is proposed to help the network predict more accurate point-wise offsets and weights. Extensive experiments demonstrate that AdaFit achieves state-of-the-art performance on both the synthetic PCPNet dataset and the real-word SceneNN dataset.</p>

<!-- <h2 id="Introduction">Method</h2>
<p>Given the input point cloud, our target is to estimate a normal for every point.</p>

<p><img src="/h3d-net/assets/images/method.png" alt="" /></p> -->

<h2 id="Result comparison">Results</h2>

<!-- <p>Results for PCPNet dataset and real-word SceneNN datasets.</p> -->


<!-- <p>Next, we show full head 3D reconstructions from only 3 input images. In these examples, the camera poses have been regressed using a pre-trained <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/GMDL/Ramon_Hyperparameter-Free_Losses_for_Model-Based_Monocular_Reconstruction_ICCVW_2019_paper.pdf">MRL model</a>, which minimizes the reprojection error. The masks have been estimated using <a href="https://arxiv.org/pdf/2005.09007.pdf">U2Net</a> and then have been manually refined.</p> -->

<p align="center">
  <img src="./images/PCPNet_error_map.png" width="700" />
</p>
<p align="center">
    <img src="./images/indoor.png" width="700" />
</p>
<!-- <p>We also provide a qualitative and quantitative comparison with respect to <a href="https://arxiv.org/abs/2003.09852">IDR</a> varying the number of available views. Note how H3D-Net effectively finds realistic and detailed solutions in both few-shot and many-shot scenarios.</p>

<p align="center">
  <img src="assets/images/h3dnet-idr.gif" />
</p> -->

<!-- <h2 id="downstreams comparison">Results</h2> -->

<p align="center">
    <video width="700" controls>
        <source src="./images/Column_surface.mp4" type="video/mp4"> 
    </video>
    <!-- <img src="./images/Column_surface.gif" width="700" /> -->
  </p>

  <p align="center">
    <video width="700" controls>
        <source src="./images/denoise.mp4" type="video/mp4"> 
    </video>
    <!-- <img src="./images/Column_surface.gif" width="700" /> -->
  </p>
  <!-- <p align="center">
      <img src="./images/denoise.mp4" width="700" />
  </p> -->

<h2 id="related-work">Related work</h2>

<ol>
  <li><a href="https://arxiv.org/abs/1710.04954">PCPNET: Learning Local Shape Properties from Raw Point Clouds (2018)</a></li>
  <li><a href="https://arxiv.org/abs/1904.07172">Deep Iterative Surface Normal Estimation (2020)</a></li>
  <li><a href="https://arxiv.org/abs/2003.10826">DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares (2020)</a></li>
</ol>

<!-- <h2 id="bibtex">BibTeX</h2> -->

<!-- <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{ramon2021h3dnet
  author    = {Ramon, Eduard and Triginer, Gil and Escur, Janna and Pumarola, Albert and Garcia, Jaime and Giro-i-Nieto, Xavier and Moreno-Noguer, Francesc},
  title     = {H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction},
  journal   = {arXiv preprint arXiv:2107.12512},
  year      = {2021},
}
</code></pre></div></div> -->


      <footer class="site-footer">
        we use the code from https://crisalixsa.github.io/h3d-net/
      </footer>
    </main>
  </body>
</html>
